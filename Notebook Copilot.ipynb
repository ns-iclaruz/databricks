{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cf05b9-a2dd-4568-865a-b4fde4fcde0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark sample: Comprehensive workflow demonstration (100+ lines)\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d68789-fa1c-41c6-bc42-a2490de7607f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Databricks Sample Workflow\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"joining_date\", StringType(), True)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9d0428-9188-48dd-8e59-f256f7156672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Hannah\", \"Ian\", \"Jane\"]\n",
    "departments = [\"HR\", \"Engineering\", \"Finance\", \"Marketing\"]\n",
    "data = [(i, name1s[i % len(names)], random.choice(departments), round(random.uniform(40000, 120000), 2),\n",
    "         datetime(2020 + (i % 5), random.randint(1, 12), random.randint(1, 28)).strftime(\"%Y-%m-%d\"))\n",
    "        for i in range(1, 101)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.createOrReplaceTempView(\"employee_data\")\n",
    "\n",
    "# Show schema and sample\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e039de9e-a140-4b9a-8e62-7c3d077aeb4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregations\n",
    "salary_summary = df.groupBy(\"department\").agg(\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.min(\"salary\").alias(\"min_salary\")\n",
    ")\n",
    "salary_summary.show()\n",
    "\n",
    "# Window function: rank employees by salary per department\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "ranked_df = df.withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "ranked_df.show()\n",
    "\n",
    "# Filter top earners per department\n",
    "top_earners = ranked_df.filter(\"rank <= 3\")\n",
    "top_earners.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa3a6c7-0e73-4fd4-88db-7b5463bbe0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add calculated column\n",
    "with_bonus = df.withColumn(\"bonus\", F.col(\"salary\") * 0.1)\n",
    "with_bonus.show()\n",
    "\n",
    "# UDF example: Categorize salary band\n",
    "def salary_band(salary):\n",
    "    if salary < 50000:\n",
    "        return \"Low\"\n",
    "    elif salary < 90000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "salary_band_udf = udf(salary_band, StringType())\n",
    "banded_df = df.withColumn(\"salary_band\", salary_band_udf(F.col(\"salary\")))\n",
    "banded_df.show()\n",
    "\n",
    "# Join with bonus table\n",
    "joined_df = banded_df.join(with_bonus.select(\"id\", \"bonus\"), on=\"id\")\n",
    "joined_df.show()\n",
    "\n",
    "# Write to Delta Lake\n",
    "delta_path = \"/tmp/delta/employee_data\"\n",
    "joined_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Read from Delta and query\n",
    "delta_df = spark.read.format(\"delta\").load(delta_path)\n",
    "delta_df.createOrReplaceTempView(\"delta_employees\")\n",
    "\n",
    "# SQL transformation\n",
    "high_salary_sql = spark.sql(\"\"\"\n",
    "SELECT name, department, salary, bonus\n",
    "FROM delta_employees\n",
    "WHERE salary_band = 'High'\n",
    "ORDER BY salary DESC\n",
    "\"\"\")\n",
    "high_salary_sql.show()\n",
    "\n",
    "# Cleanup for re-runs\n",
    "dbutils.fs.rm(delta_path, recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f19c1b-cacc-4d52-8ded-9a5de53a366c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook Copilot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
